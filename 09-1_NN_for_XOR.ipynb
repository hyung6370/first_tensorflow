{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b95319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1acfde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR with logistic regression\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e502d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f2efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2838ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 12:00:10.726812: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-28 12:00:10.733013: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.77113575 [[-1.5727118]\n",
      " [ 0.2781593]]\n",
      "100 0.7212909 [[-0.9361392 ]\n",
      " [ 0.07876157]]\n",
      "200 0.7034631 [[-0.5646738]\n",
      " [-0.0179355]]\n",
      "300 0.6970829 [[-0.34365982]\n",
      " [-0.05083464]]\n",
      "400 0.69471705 [[-0.21158858]\n",
      " [-0.05503368]]\n",
      "500 0.69379807 [[-0.13185811]\n",
      " [-0.04820349]]\n",
      "600 0.6934252 [[-0.08312897]\n",
      " [-0.03843584]]\n",
      "700 0.69326854 [[-0.05296853]\n",
      " [-0.0290922 ]]\n",
      "800 0.69320095 [[-0.03407297]\n",
      " [-0.02131776]]\n",
      "900 0.69317126 [[-0.0221008 ]\n",
      " [-0.01528675]]\n",
      "1000 0.69315815 [[-0.01443782]\n",
      " [-0.01079767]]\n",
      "1100 0.69315207 [[-0.00948886]\n",
      " [-0.00754427]]\n",
      "1200 0.6931494 [[-0.00626776]\n",
      " [-0.00522899]]\n",
      "1300 0.69314814 [[-0.00415742]\n",
      " [-0.0036025 ]]\n",
      "1400 0.69314766 [[-0.00276709]\n",
      " [-0.00247059]]\n",
      "1500 0.6931473 [[-0.00184683]\n",
      " [-0.00168843]]\n",
      "1600 0.6931473 [[-0.0012354 ]\n",
      " [-0.00115079]]\n",
      "1700 0.69314724 [[-0.0008279 ]\n",
      " [-0.00078271]]\n",
      "1800 0.6931472 [[-0.00055564]\n",
      " [-0.00053149]]\n",
      "1900 0.69314724 [[-0.00037334]\n",
      " [-0.00036044]]\n",
      "2000 0.6931472 [[-0.00025109]\n",
      " [-0.0002442 ]]\n",
      "2100 0.6931472 [[-0.00016898]\n",
      " [-0.0001653 ]]\n",
      "2200 0.6931472 [[-0.00011379]\n",
      " [-0.00011182]]\n",
      "2300 0.6931472 [[-7.6662735e-05]\n",
      " [-7.5606295e-05]]\n",
      "2400 0.6931472 [[-5.1663086e-05]\n",
      " [-5.1098355e-05]]\n",
      "2500 0.6931472 [[-3.4826255e-05]\n",
      " [-3.4525288e-05]]\n",
      "2600 0.6931472 [[-2.3481265e-05]\n",
      " [-2.3315884e-05]]\n",
      "2700 0.6931472 [[-1.5833248e-05]\n",
      " [-1.5740879e-05]]\n",
      "2800 0.6931472 [[-1.0678933e-05]\n",
      " [-1.0626799e-05]]\n",
      "2900 0.69314724 [[-7.1898289e-06]\n",
      " [-7.1719714e-06]]\n",
      "3000 0.6931472 [[-4.8525826e-06]\n",
      " [-4.8391958e-06]]\n",
      "3100 0.6931472 [[-3.2641219e-06]\n",
      " [-3.2596759e-06]]\n",
      "3200 0.6931472 [[-2.2024130e-06]\n",
      " [-2.2009472e-06]]\n",
      "3300 0.6931472 [[-1.4670388e-06]\n",
      " [-1.4670629e-06]]\n",
      "3400 0.6931472 [[-9.916898e-07]\n",
      " [-9.917139e-07]]\n",
      "3500 0.6931472 [[-7.0558633e-07]\n",
      " [-7.0561043e-07]]\n",
      "3600 0.6931471 [[-4.515207e-07]\n",
      " [-4.515448e-07]]\n",
      "3700 0.6931472 [[-2.950579e-07]\n",
      " [-2.950820e-07]]\n",
      "3800 0.6931472 [[-2.1384677e-07]\n",
      " [-2.1387088e-07]]\n",
      "3900 0.6931472 [[-1.3934068e-07]\n",
      " [-1.3936479e-07]]\n",
      "4000 0.6931472 [[-7.2285204e-08]\n",
      " [-7.2309305e-08]]\n",
      "4100 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "4200 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "4300 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "4400 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "4500 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "4600 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "4700 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "4800 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "4900 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5000 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5100 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5200 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5300 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5400 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5500 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5600 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5700 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5800 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "5900 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6000 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6100 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6200 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6300 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6400 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6500 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6600 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6700 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6800 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "6900 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7000 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7100 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7200 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7300 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7400 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7500 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7600 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7700 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7800 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "7900 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8000 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8100 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8200 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8300 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8400 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8500 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8600 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8700 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8800 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "8900 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9000 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9100 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9200 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9300 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9400 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9500 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9600 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9700 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9800 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "9900 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "10000 0.6931472 [[-4.3972964e-08]\n",
      " [-4.3997066e-08]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc018d91",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2697c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR with logistic regression\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf10123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3d6d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8543ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1959145 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "100 0.68614495 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "200 0.6707504 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "300 0.657032 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "400 0.6410949 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "500 0.6201204 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "600 0.5920982 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "700 0.55717933 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "800 0.5172275 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "900 0.4732644 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1000 0.42607498 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1100 0.37736756 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1200 0.32947004 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1300 0.28462592 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1400 0.2444335 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1500 0.2096299 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1600 0.18020895 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1700 0.15569723 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1800 0.13541587 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "1900 0.11865602 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2000 0.10477097 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2100 0.09321016 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2200 0.0835227 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2300 0.0753469 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2400 0.06839581 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2500 0.0624422 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2600 0.057306334 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2700 0.052845526 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2800 0.048945837 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "2900 0.045515846 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3000 0.04248167 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3100 0.039783306 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3200 0.03737156 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3300 0.035205998 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3400 0.033252977 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3500 0.031484492 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3600 0.029876988 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3700 0.028410716 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3800 0.02706878 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "3900 0.025836794 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4000 0.024702447 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4100 0.023655197 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4200 0.022685753 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4300 0.021786202 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4400 0.020949565 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4500 0.020169808 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4600 0.019441511 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4700 0.01875995 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4800 0.018120969 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "4900 0.017520864 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5000 0.016956335 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5100 0.01642446 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5200 0.01592249 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5300 0.015448187 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5400 0.014999327 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5500 0.014574051 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5600 0.014170542 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5700 0.013787326 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5800 0.013422882 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "5900 0.0130759515 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6000 0.0127453245 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6100 0.012429927 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6200 0.012128764 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6300 0.011840904 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6400 0.01156555 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6500 0.011301925 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6600 0.011049309 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6700 0.010807032 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6800 0.010574483 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "6900 0.010351145 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7000 0.010136498 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7100 0.009930012 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7200 0.009731318 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7300 0.009539945 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7400 0.009355485 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7500 0.009177631 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7600 0.009006007 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7700 0.008840349 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7800 0.008680296 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "7900 0.008525666 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8000 0.008376079 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8100 0.008231368 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8200 0.008091335 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8300 0.007955693 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8400 0.007824277 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8500 0.0076969163 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8600 0.0075734016 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8700 0.0074535664 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8800 0.0073372577 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "8900 0.0072243554 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9000 0.0071146777 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9100 0.007008089 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9200 0.006904513 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9300 0.0068037836 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9400 0.00670578 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9500 0.006610426 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9600 0.006517617 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9700 0.006427246 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9800 0.0063392078 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "9900 0.0062534115 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "10000 0.006169842 [[ 0.1467494]\n",
      " [-0.9478915]]\n",
      "\n",
      "Hypothesis:  [[0.00493148]\n",
      " [0.9932386 ]\n",
      " [0.99409854]\n",
      " [0.00700773]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
